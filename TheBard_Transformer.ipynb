{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b24cf43-f6bb-46d9-ab97-d6bf488748f3",
   "metadata": {},
   "source": [
    "Import our dependencies and the Minitorch from my github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "444d1968-4be8-4511-8dd8-6bc41fdeea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from engine.minitorch import Tensor,optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46196ca-0f6c-461f-9765-361da3050528",
   "metadata": {},
   "source": [
    "I will put a seed right now for the reproducibility of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4bab7bf-933e-4725-9c3f-3fcb1994cd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3329)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbf99ea-cc91-4740-b81b-59d18ad6a070",
   "metadata": {},
   "source": [
    "This are the variables that we need in order to create the data folder with the dataset inside,because I can't upload the repository with the dataset of Shakespeare pre-charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b9c463-6716-482f-aeb4-c4176f745042",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"data\"\n",
    "file_path = os.path.join(data_directory, \"input.txt\")\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d34f5b2-205b-43bb-b19a-3b7b92ea9a7e",
   "metadata": {},
   "source": [
    "This will create a data folder if it doesn't exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21fe0ac9-d73b-4ad2-9663-a7ac7d2959a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_directory):\n",
    "    \n",
    "    os.makedirs(data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2029296c-2484-4042-8eea-efeceb40f59b",
   "metadata": {},
   "source": [
    "if the inside of the data folder that we just created is empty we throw a request to the url for the dataset of Shakespeare of the github of Karpathy,raise for status to inform us what happened with the request and then open it indicating to the computer that need to transform it binary to avoid missinterpretations of characters and then finally write it down to the \"input.txt\" inside our data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bd91d87-5ae3-4451-9de0-cbebec12745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(file_path):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    response.raise_for_status()\n",
    "\n",
    "\n",
    "    with open(file_path, \"wb\") as f:\n",
    "\n",
    "        f.write(response.content)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12153e54-7ab3-4198-8aa7-5ef9a727c813",
   "metadata": {},
   "source": [
    "Here what we do is open the input.txt of Shakespeare preaviously stored in the data folder and read it,the encoding utf-8 is the standard to read special characters like emojis,commas,dots,etc.\\\n",
    "Then the text variable is just all that concatenated ,like a list of all Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f47f998c-44ef-4aa8-a33f-bc8849130ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f21a52b-7b45-4a47-aa01-c0d06a6e392f",
   "metadata": {},
   "source": [
    "First we convert the text into a set,this will give us every character inside the text.\\\n",
    "Then we transform that set into a list to be able to sorted in order that the same character never change its index.\\\n",
    "Lastly we can see the len of the vocabulary and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d058254-74cd-4627-afaf-ae7b4608fe56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "\n",
    "print(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c839db03-2a74-4bd3-96cc-8b8f0d571a17",
   "metadata": {},
   "source": [
    "To tokenize them we just create two dictionaries, String to Integer(stoi) and Integer to String(itos).\n",
    "\n",
    "One will have as key_value the character and as the value the index of that character in the list of chars.\n",
    "\n",
    "And the other one will have its key_value the index and as value the character of that index in the list of chars.\n",
    "\n",
    "This is exactly what we want because we use the list of characters and its indexes to transform them into integers and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f4f99df-69e2-49aa-9ece-bd8c4b6aefa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "\n",
    "itos = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f25d294-37ba-482c-9530-017d25e5e33a",
   "metadata": {},
   "source": [
    "Now here is the actual encoding and decoding.\\\n",
    "We gonna use lambda function because this is simple and with lambda we avoid writing a def just for this.\n",
    "\n",
    "encoder = here we pass to lamba a string(s) then we ask for its correspondent value inside the stoi dictionary.\n",
    "\n",
    "decoder = this will receive a list of numbers,search what character correspond to that integer inside of itos and concatenate them into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc49c95f-c212-48a6-b756-8d3808cd6720",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = lambda s:[stoi[c] for c in s]\n",
    "\n",
    "decoder = lambda l: ''.join([itos[i]for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571b3444-67a8-46e1-9a12-0a6281f9d1a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
