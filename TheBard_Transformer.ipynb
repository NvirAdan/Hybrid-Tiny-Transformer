{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b24cf43-f6bb-46d9-ab97-d6bf488748f3",
   "metadata": {},
   "source": [
    "Import our dependencies and the Minitorch from my github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "444d1968-4be8-4511-8dd8-6bc41fdeea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from engine.minitorch import Tensor,optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46196ca-0f6c-461f-9765-361da3050528",
   "metadata": {},
   "source": [
    "I will put a seed right now for the reproducibility of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4bab7bf-933e-4725-9c3f-3fcb1994cd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3329)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbf99ea-cc91-4740-b81b-59d18ad6a070",
   "metadata": {},
   "source": [
    "This are the variables that we need in order to create the data folder with the dataset inside,because I can't upload the repository with the dataset of Shakespeare pre-charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b9c463-6716-482f-aeb4-c4176f745042",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"data\"\n",
    "file_path = os.path.join(data_directory, \"input.txt\")\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d34f5b2-205b-43bb-b19a-3b7b92ea9a7e",
   "metadata": {},
   "source": [
    "This will create a data folder if it doesn't exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21fe0ac9-d73b-4ad2-9663-a7ac7d2959a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_directory):\n",
    "    \n",
    "    os.makedirs(data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2029296c-2484-4042-8eea-efeceb40f59b",
   "metadata": {},
   "source": [
    "if the inside of the data folder that we just created is empty we throw a request to the url for the dataset of Shakespeare of the github of Karpathy,raise for status to inform us what happened with the request and then open it indicating to the computer that need to transform it binary to avoid missinterpretations of characters and then finally write it down to the \"input.txt\" inside our data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bd91d87-5ae3-4451-9de0-cbebec12745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(file_path):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    response.raise_for_status()\n",
    "\n",
    "\n",
    "    with open(file_path, \"wb\") as f:\n",
    "\n",
    "        f.write(response.content)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12153e54-7ab3-4198-8aa7-5ef9a727c813",
   "metadata": {},
   "source": [
    "Here what we do is open the input.txt of Shakespeare preaviously stored in the data folder and read it,the encoding utf-8 is the standard to read special characters like emojis,commas,dots,etc.\\\n",
    "Then the text variable is just all that concatenated ,like a list of all Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f47f998c-44ef-4aa8-a33f-bc8849130ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f21a52b-7b45-4a47-aa01-c0d06a6e392f",
   "metadata": {},
   "source": [
    "First we convert the text into a set,this will give us every character inside the text.\\\n",
    "Then we transform that set into a list to be able to sorted in order that the same character never change its index.\\\n",
    "Lastly we can see the len of the vocabulary and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d058254-74cd-4627-afaf-ae7b4608fe56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "\n",
    "print(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c839db03-2a74-4bd3-96cc-8b8f0d571a17",
   "metadata": {},
   "source": [
    "To tokenize them we just create two dictionaries, String to Integer(stoi) and Integer to String(itos).\n",
    "\n",
    "One will have as key_value the character and as the value the index of that character in the list of chars.\n",
    "\n",
    "And the other one will have its key_value the index and as value the character of that index in the list of chars.\n",
    "\n",
    "This is exactly what we want because we use the list of characters and its indexes to transform them into integers and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f4f99df-69e2-49aa-9ece-bd8c4b6aefa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "\n",
    "itos = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f25d294-37ba-482c-9530-017d25e5e33a",
   "metadata": {},
   "source": [
    "Now here is the actual encoding and decoding.\\\n",
    "We gonna use lambda function because this is simple and with lambda we avoid writing a def just for this.\n",
    "\n",
    "encoder = here we pass to lamba a string(s) then we ask for its correspondent value inside the stoi dictionary.\n",
    "\n",
    "decoder = this will receive a list of numbers,search what character correspond to that integer inside of itos and concatenate them into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc49c95f-c212-48a6-b756-8d3808cd6720",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = lambda s:[stoi[c] for c in s]\n",
    "\n",
    "decoder = lambda l: ''.join([itos[i]for i in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571b3444-67a8-46e1-9a12-0a6281f9d1a1",
   "metadata": {},
   "source": [
    "Now that we got our encoder and decoder que can transform the text into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac633cf-f554-4669-9775-1709b1805d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.array(encoder(text), dtype=np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1be1b23-d448-47d1-9fdf-441edda5662e",
   "metadata": {},
   "source": [
    "Here we gonna take the 90% of the total data(len of total_data * 0.9).\\\n",
    "And then we use that number to divide the data into train_data and test_data.\n",
    "\n",
    "train_data being the first 90% of the data.\\\n",
    "test_data being the last 10% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeebbcaa-77d0-4afe-831f-43515c3e75f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(all_data))\n",
    "\n",
    "train_data = all_data[:n]\n",
    "test_data = all_data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f178cd8d-7932-4831-9657-d15972068203",
   "metadata": {},
   "source": [
    "This is momentary and its something that you can change to have a different speed or precision in training.\\\n",
    "I realize that I need this 2 variables because of the creation of the parameters for the feed-foward network later on in the creation of the foward pass functions,so I decide to put this right up here(In the training preparation you can change the values according to how you want to train the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642103fe-338f-488c-9cb5-90828e9e1d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112ebfce-b3ff-49dc-90b2-96d96d57201c",
   "metadata": {},
   "source": [
    "Here is the function to create batches because for training you can't pass all the data at once,explanation = Self-explanatory, just don't.\n",
    "\n",
    "For that reason we create this function passing as arguments the split which is a string.\\\n",
    "The batch_size which is one of the dimensions and the block_size which is the other dimension.\n",
    "\n",
    "First we declare the variable data, this variable will be equal to the train_data if when we pass the split(the string parameter) we said \"train\" otherwise the variable data will be equal to the test_data.\n",
    "\n",
    "Now we gonna create the randomness in the batches in order to train better the model,using numpy,we tell it to give us a random number between 0 and the total of the data minus the block size so it doesn't give us por example the last 2 tokens in the data and broke our block size.\\\n",
    "And then we also pass the batch_size to tell numpy that we want more than a single number.\n",
    "\n",
    "Next we gonna create our Tensors2D(block_size,batch_size).\\\n",
    "To create our inputs (x) we need to put them in a stack in order to create the mesh/matrix so using and excel sheet as analogy you can interpret the block size as the how large will we the rows and the batch_size as how large will be the columns(for example, if we pass block_size=8 and batch_size=4 the tensor that we gonna get is a 4x8 matrix,4 batches with a lenght of 8 values each).\\\n",
    "Once acquire the inputs, the targets(y) follow the same creation logic but +1.It will be the same size as the tensor \"X\".\n",
    "Example: if tensor start with something like  43,56,21 then tensor Y will be like 56,21,33 .\n",
    "\n",
    "Lastly we just return both the inputs and targets tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b010bd3-160b-46fe-8e72-2769459f3182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(split,batch_size,block_size):\n",
    "\n",
    "    data = train_data if split == \"train\" else test_data\n",
    "\n",
    "    idx = np.random.randint(0,len(data) - block_size,batch_size)\n",
    "\n",
    "    x = np.stack(data[i:i+block_size] for i in idx)\n",
    "\n",
    "    y = np.stack(data[i + 1 : i + block_size + 1] for i in idx)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60f744d-5ba2-4acc-a960-4e0386a5ba3c",
   "metadata": {},
   "source": [
    "Here we have our variables to be able to create the table of embedding.\\\n",
    "We will use this to transform the 65 posibles tokens into vectors of random values(one vector per char) to be able to do maths operations with the vectors.\n",
    "\n",
    "First we take the vocab_size.\\\n",
    "Then we assing a variable called numbers of embedding which is arbitrary,for a model like this I recommend 128 because is a tiny model,every of the 128 will give to the model context of what is about the token, for example, if the token 5 represent the letter \"a\" the table of embedding with each of the 128 will give us an adjective of it: \"vocal\",\"minuscule\".\n",
    "\n",
    "\n",
    "Then we initialize the variable table creating a tensor2D of random numbers,each row will represent 1 token(65 row because of the vocab_size) and every column is value of embedding for that token(128 embeddings per token).Creating a tensor2D of (65,128).\\\n",
    "We just multiply those values by the Xavier formula that is commonly used for Softmax/Sigmoid/Tanh because the randomness is too high for the model to start learning right away,it could probably(and luckily if doesn't explode)take a lot of epochs start learning.\n",
    "\n",
    "By the way,we created this Variables outside the get_embeddings function because we need to train this random values,if we initialize them inside the function they will be created every time we call the function and never train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eafe2d-b362-416e-bc53-cc72454aab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = vocab_size # Just to bring it closer we already know that they are 65 characters\n",
    "\n",
    "n_embd = 128\n",
    "\n",
    "\n",
    "token_table = np.random.randn(vocab_size,n_embd) * np.sqrt(2/n_embd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1fe52e-1777-4889-a32b-882657d42376",
   "metadata": {},
   "source": [
    "This short function is the embedding of our tokens,is short because Numpy does the hard lifting for us creating the new tensor and all that.\n",
    "\n",
    "When we call the function we pass our batches of tokens for example (4x8) and we also pass the table of embeddings of random values(65,128).\\\n",
    "What Numpy does is create a new Tensor, but this tensor is a Tensor3D.\\\n",
    "When we said use the inputs as index of table we practically say,from the embeddings table put in a new tensor the rows of every value in the block_size of the batch_size giving us a Tensor3D of (4,8,128).\\\n",
    "In an excel sheet example in this new tensor we got: the 128 embeddings columns(n_embd=128) for each row of the 8 rows(block_size=8) and once we got that tensor2D we need to stack 4 tensor2D like those(batch_size=4) on top of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663ad236-4a4f-48d4-bb2b-ceff92cb18d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  def get_embeddings(xb,token_table):\n",
    "#      return token_table[xb]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36d3229-81e9-465f-ace5-77494129fc04",
   "metadata": {},
   "source": [
    "Same as the token_table we iniliazite a tensor2D but instead of using the vocab_size we use the block_size(Also multiplying by the Xavier formula)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8de48a-b32a-4bcd-950d-906ec3415207",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_table = np.random.randn(block_size,n_embd) * np.sqrt(2/n_embd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aedcdfa-ae39-4069-bb3a-933a67694d44",
   "metadata": {},
   "source": [
    "I encourage you to learn the theory behind this positional encoding,is facinating see how words can follow a sinusoidal wave to know time and position of each embedding(maybe in future projects I'll implement this advance,more mathematical version on a transformer,which is the original way suggested by the paper \"Atenttion is all you need\",extrapolation, \"infinite\" positions),For now I'll stick with the GPT way(Learned positional embeddings)which is just another tensor like the \"embeddings_table\" that add time and position with random numbers that later on the model will learn.\\\n",
    "Because the model without the positional encoding will think that \"The dog bites the man\" is the same as \"The man bites the dog\".\n",
    "\n",
    "\n",
    "Here we take as parameters the stack batch(4,8) the token table and pos_table that we previusly have created.\n",
    "\n",
    "For future operations I'll take the shape of xb .\\\n",
    "Now to make things more modular just in case that in the future I want a block_size of 16 instead of one of 8 like right now,we create an numpy array with the length of our block_size(T=8).\n",
    "\n",
    "Then we have the token_table(65,128) but we need a tensor3D of (4,8,128 like the token_embeddings of the previous function).\\\n",
    "so we use the same fancy indexing trick to give us the tensor3D.(after a conclusion I find the previous functon redundant and I comment it, so it doesn't affect when running all the cells,just in case;But I will leave it right there because I see it as a sign of learn not as a mistake).\n",
    "\n",
    "Then this next part is just to \"name\" put \"labels\" to every row in the pos_table,for example the first row of pos_table will be named position:0(this a analogy of how the model will think) and the next row position:1, this will continue until position:7 because the block_size only have 8 positions.\\\n",
    "This will give to the model a perception of time/space because this value will be train to tell the posible position of the token after seeing all the Shakespeare train data.\n",
    "\n",
    "Next and finally is just the summ of the token_embd which give us \"characteristics\" of the token with the pos_embd that will give us \"time/space\" of the token,if you are wondering how two tensor with different sizes can do an addition to each other,the answer is : Because of numpy,numpy is doing the broadcasting so we don't have problems with the dimension,he es just doing again and again the addition of the tensor2D(pos_embd) the amount of batches of the tensor3D(4),so we ended up with a (4,8,128) tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ece254-f3ac-4d83-8aa8-6f919da0e34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(xb,token_table,pos_table):\n",
    "\n",
    "    B,T = xb.shape\n",
    "\n",
    "    pos_idx = np.arange(T)\n",
    "\n",
    "    token_embd = token_table[xb] # .data because that is the numpy array inside the Tensor of my Minitorch\n",
    "\n",
    "    pos_embd = pos_table[pos_idx]\n",
    "\n",
    "    x = token_embd + pos_embd\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae60ad25-f99b-4a7c-890e-7951b5a71112",
   "metadata": {},
   "source": [
    "I'm using my minitorch but the \"new parts\" of the Transformer(positional encoding,multi-head atenttion,etc) I'll do it manual(including backprop).\\\n",
    "So for that reason I'll need a Softmax function.\\\n",
    "Quick recap(I've been explaining it in every project):\n",
    "\n",
    "1 - Take the max number of the array so it doesn't explode doing the exponentiation.\\\n",
    "2 - Before the exponentiation to the array subtract the max value of the array to the array ,then exponentiate it.\\\n",
    "3 - The Result is just the array exponentiated divided the summatory of the array exponentiated.\n",
    "\n",
    "Btw Softmax is mostly applied in the last dimension so the axis = -1 should be a standard and is how I did it in my Minitorch.\\\n",
    "(Softmaxx with double \"xx\" is on purpose to make sure that it doesn't do a conflict with the Softmax function of my minitorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e9bf73-2383-43f4-a5e8-9863b1269b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmaxx(inputs):\n",
    "\n",
    "    max_num = np.max(inputs, axis = -1, keepdims=True)\n",
    "\n",
    "    scores = np.exp(inputs - max_num)\n",
    "\n",
    "    out = scores / np.sum(scores, axis = -1, keepdims=True)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b72246-6f3d-45a9-b7bd-ae18234ff6d8",
   "metadata": {},
   "source": [
    "And I'll do its backpropagation that is something that I also did in my Minitorch.\\\n",
    "Its just using the trick with the formula of the Jacobian.\n",
    "\n",
    "I'll not go deep into mathematical reasons but the derivative of Softmax is just the out of the foward pass of the softmax multiplied by the substraction of the summatory of the gradient multiplied by the out of the foward to the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d657c20-4ef7-4e08-99ba-1e391a551d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_Softmax(out,gradient):\n",
    "\n",
    "    summatory_gradout = np.sum(gradient * out, axis = -1, keepdims=True)\n",
    "\n",
    "    dSoftmax = out * (gradient - summatory_gradout)\n",
    "\n",
    "    return dSoftmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c76dc0c-5d56-419a-8af9-cb7e04891179",
   "metadata": {},
   "source": [
    "Here,is important to first of all create our parameters (Query, Key, Value) outside the function.\\\n",
    "To create them we just gonna use the embeddings dimmensions for something that I'll explain in the multi_head_atenttion function.\n",
    "Like always when you create a tensor with random values you need to multiply them by the Xavier or He init formula(The one that comes better for the reason of what you gonna use that tensor with random values).\n",
    "\n",
    "Note:The Wo variable is also important and is written in the paper \"Atenttion is all you need\" is like a \"mixxer\" of the reponses of the heads to make the output have more coherence,but its responsabilities are not as \"important\" as the other 3,just for that reason to clarify the panoram I'll treat it different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8a5d80-9a5c-4528-8274-111dde85b1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wq = np.random.randn(n_embd, n_embd) * np.sqrt(1 / n_embd)\n",
    "Wk = np.random.randn(n_embd, n_embd) * np.sqrt(1 / n_embd)\n",
    "Wv = np.random.randn(n_embd, n_embd) * np.sqrt(1 / n_embd)\n",
    "\n",
    "Wo = np.random.randn(n_embd, n_embd) * np.sqrt(1 / n_embd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29f7ee0-fb9f-48a2-8cb1-f14da46640c0",
   "metadata": {},
   "source": [
    "Here is the heart of the transformer,The MHA(Multi Head Atenttion),I've divide it by sections for me to be easier to explain:\n",
    "\n",
    "Section 0: This is just preparation,we take the shape of our inputs and create the size of every of the 4 heads,size which will be 32 embeddings each head.\n",
    "\n",
    "\n",
    "Section 1: Here we combine the data of our input with the data of ours parameters just to differentiate them of the function of what each one does.\\\n",
    "Creating the variables Q (query), K(key), V(value).\n",
    "\n",
    "\n",
    "Section 2: This step is simply to organize them in orden to be able to do its respective function down the road.\\\n",
    "First we take the Q,K,V data from the previous section and reshape them to become a tensor4D(4,8,4,32) and then transpose them to be able to work with the 32 embeddings(1 head) of each one of the token inside the block_size.\n",
    "\n",
    "\n",
    "Section 3: Here is when we use the differentiation of each one.\\\n",
    "Commonly I think about the query as the \"signal\" that the token who wants to predict the next one emits to start searching in a library(the \"What i want\").\\\n",
    "The key I interpret them as Label/Name of the books (each token have 4 books describing it,the heads).\\\n",
    "And finally the value as the content of the book.\n",
    "\n",
    "Our purpose is create 4 new books(One Token) so in order to create them we going to search in the library and take the content that we want for those next new 4 books.\n",
    "\n",
    "So in this section we compare every label of the book in the library with what we want to know how much of every book we going to take.\\\n",
    "The formula will be Q*K.transpose for that searching,transpose because (excluding the dimension that we don't care right now) we got Q=(8,32) and K=(8,32) so in orden to do the matmul we transpose the K ->(32,8) and now we can start \"searching\" (8,32)*(32,8).\n",
    "\n",
    "Finally for this section, we going to do the scaling to the weights(wei), This is just for the Softmax so it doesn't explodes with numbers to high.\\\n",
    "Like lowering the volume of the values so Softmax doesn't get stunned.\n",
    "\n",
    "\n",
    "Section 4: We need to do Softmax in order to be able to know how many of each of the heads we need to take to predict the next token correctly.\\\n",
    "For that reason we do Softmax in the last dimension,but something that happens when we do the Softmax is that the Softmax also takes in consideration the next tokens(the ones that we want to predict),for example,if we are in position 5 and we want to predict the token of position 6 we going to do a Softmax to all the tokens from position 0 to position 7 and if you notice that is an issue because with Softmax the model will cheat because it will know already what token is next.\\\n",
    "The solution for that is an Atenttion mask,what is that you will ask,well you can see it as phantom tensor that we put on top of our tensor of tokens,this atenttion mask will have a triangular form that we gonna fill with 1s an the rest with either \"-inf\" or a number extremely low.\\\n",
    "What this will do?This atenttion mask will prevent the model to cheat because the mask will replace our tensor with numbers in a form that every time we want to predict the next token the values of the next token will be 0 cause of the traingular form of the mask, this will tell Softmax that we don't want to take the any information of future tokens in order to be able to predict it using the past tokens only.\\\n",
    "So in the code we first create the mask using numpy and given the dimensions of our block size (8,8).\\\n",
    "Then using again numpy we can do this multiplication that I've talked about earlier to assing the values of wei where the ones are and the positions without 1 we fill it with negative infinite(in this case I'll use negative 1 billon just to make sure that I don't will fine any computer error down the road,which it could be the case if I leave it with negative infinite).\n",
    "\n",
    "\n",
    "Section 5: Now that we are sure that the model will not cheat we can just do the Softmax.\n",
    "\n",
    "\n",
    "Section 6: Here because the softmax already tell how many percentage of each head we want to take to have a good prediction we just do a matmul between how many of the values we want and the respective values of each head.\n",
    "\n",
    "\n",
    "Section 7: Now that we got our values we gonna transpose and reshape them in order to have the same \"form\" of the tensor x that was our input to the function,to make things easier for future functions that it going to pass through.\n",
    "\n",
    "\n",
    "Section 8: Finally and now that we got our results of the multihead atenttion we can \"mix them\".\\\n",
    "Every head you can see them as a proffesional in differents aspects of the tokens,one head can be an proffesional in semantic,other head and professional in sustantives,etc.\\\n",
    "So our output tensor will be just a concatenation of different proffesional opinions of what will be the next token.\\\n",
    "The heads are independent so the concatenation will sound like different peoples doing the same project and just combining they parts to the same project.This matmul will prevent that,it will \"mix\" the results (the opinion of each proffesional) to make the final output sound more coherent.\n",
    "\n",
    "\n",
    "Section 9: While doing the backpropagation of the MHA I learned that I'll need the cache of the MHA ,so I put this section later in the creation of the model because without this information that I saved in the cache variable I can't do the backpropagation of the MHA.\\\n",
    "Is not the first time that I save a cache variable,for example in the CNN from scratch in the maxpool function you need to save the cache to know how was the size before to make the tensor of the gradient become a tensor with the correct size and dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1eac042-67e6-4064-9c42-96b5db1246ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_atenttion(x, Wq, Wk, Wv, Wo, heads):\n",
    "\n",
    "    # Section 0\n",
    "    \n",
    "    batch_size, block_size, total_embeddings = x.shape # The classic (B, T, C)\n",
    "\n",
    "    head_size = total_embeddings // heads \n",
    "\n",
    "    \n",
    "    # Section 1\n",
    "    \n",
    "    Q = x @ Wq\n",
    "\n",
    "    K = x @ Wk\n",
    "\n",
    "    V = x @ Wv\n",
    "\n",
    "    \n",
    "    # Section 2\n",
    "    \n",
    "    Q = Q.reshape(batch_size, block_size, heads, head_size).transpose(0, 2, 1, 3)\n",
    "\n",
    "    K = K.reshape(batch_size, block_size, heads, head_size).transpose(0, 2, 1, 3)\n",
    "\n",
    "    V = V.reshape(batch_size, block_size, heads, head_size).transpose(0, 2, 1, 3)\n",
    "\n",
    "    \n",
    "    # Section 3\n",
    "\n",
    "    wei = Q @ K.transpose(0, 1, 3, 2)\n",
    "\n",
    "    wei = wei * (head_size**-0.5) # More fancy than \"(1 / np.sqrt(head_size))\" which is also valid.\n",
    "\n",
    "\n",
    "    \n",
    "    # Section 4\n",
    "\n",
    "    mask = np.tril(np.ones((block_size, block_size)))\n",
    "\n",
    "    wei = np.where(mask == 0, -1e9, wei) #Underflow instead of -inf.\n",
    "\n",
    "\n",
    "    \n",
    "    # Section 5 \n",
    "\n",
    "    probs = Softmax(wei)\n",
    "\n",
    "\n",
    "\n",
    "    # Section 6\n",
    "\n",
    "    out_heads = probs @ V\n",
    "\n",
    "\n",
    "    \n",
    "    # Section 7\n",
    "\n",
    "    out_concat = out_heads.transpose(0, 2, 1, 3).reshape(batch_size, block_size, total_embeddings)\n",
    "\n",
    "\n",
    "    \n",
    "    # Section 8\n",
    "\n",
    "    out = out_concat @ Wo\n",
    "\n",
    "\n",
    "\n",
    "    # Section 9\n",
    "\n",
    "    cache = (x, Wq, Wk, Wv, Wo, Q, K , V, wei, probs, out_concat, head_size)\n",
    "\n",
    "    return out,cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a9ac42-b5b2-408e-9973-4b294b148ecd",
   "metadata": {},
   "source": [
    "Initialization of the Parameters Gamma and Beta,which we will use in order to save and train them to make a more accurate model.\\\n",
    "The gamma parameter represents the Standard Deviation,which will be a tensor2D full of 1s (128,1),this can't be a tensor2D of zeros because if we multiply the information by 0 obviously the result will be 0 and we \"kill the information\".The first time it will pass as it is (because of the multiplication by 1)and then the parameter gamma will get actualize in order to pay more attention to the small values and have a more accurate standard deviation.\\\n",
    "The beta parameter represents the ideal mean that we want,it will start as a tensor2D full of 0s with the same size as the gamma tensor(128,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6aca3d-1301-4629-a1b2-b1d7cdcc2962",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = np.ones((n_embd,))\n",
    "beta = np.zeros((n_embd,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5b54e9-2d10-4d32-b3d4-52daed8c073b",
   "metadata": {},
   "source": [
    "Now that we have the \"opinions\" after we pass through the MHA we need to normalize them,because right now we can have for example: a value that is 50 and a value that is 0.01,here what is going to happen is that the higher value (50) will overlap the lower value (0.01).\\\n",
    "This is something that we don't want because both value are usefull,so in order to avoid this we will normalize them using a simple formula which is:\\\n",
    "input normalized is the substraction of the mean of the inputs to the inputs divided by the standard deviation of the inputs(the epsilon is as always just for the PC to take tiny numbers and don't convert them to 0 which is what the PC tends to do).\n",
    "\n",
    "You can see x - x_mean like the process of making the numbers go lower, this a type of normalization like we do in the Softmax function but here instead of the Maximum number we use the Mean number.\\\n",
    "Then dividing with the standard deviation is the process to make them be \"closer to each other\",for example if they were scattered from 0 to 500 this will make them be scattered from 0 to 1.\\\n",
    "This is something that you will find in every layer norm function because is something standard in all of them,the things that changes in every model tho is the gamma and beta parameters because is something that is trained as every weight in the model,changing the specific deviation and mean in each model.(We need to save the cache for the manual backpropagation of the layer norm where we gonna give to gamma and beta their respective gradients)\n",
    "\n",
    "\n",
    "PD: while I was learning about the history of this I come across something.\\\n",
    "The LN(Layer Norm)previously when the paper \"Attention is all you need\" came out,the LN was only used in the post-MHA,now in modern times the LN is used pre-MHA,models like GPT-2,GPT-3,Llama use the LN before the MHA,this is because doing it before the MHA makes the model easier to train(more stable).\\\n",
    "This doesn't change the explanation neither the formula but it changes what values the function takes and when you gonna use it in the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00977cfa-dc14-40c1-815b-259d9d85c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x,gamma,betta, eps=1e-5):\n",
    "\n",
    "    mean = x.mean(axis = -1, keepdims=True)\n",
    "\n",
    "    var = x.var(axis = -1, keepdims=True)\n",
    "\n",
    "    std = np.sqrt(var + eps)\n",
    "\n",
    "    \n",
    "    \n",
    "    x_hat = (x - mean) / std\n",
    "\n",
    "\n",
    "    out = gamma * x_hat + beta\n",
    "\n",
    "\n",
    "    cache = (x_hat,std,gamma)# All in the same variable,later we will unpack it.\n",
    "\n",
    "    \n",
    "    return out,gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad2eb3b-ea93-4582-af80-9b8947fc743d",
   "metadata": {},
   "source": [
    "This is just a \"Residual Connection\",our inputs (x) will travel across the entire transformer and after almost every function we gonna call this function that is just a addition of the previous inputs with the output of the function that have used those inputs.\\\n",
    "This will help with the gradient vanish cause by the multiplicatiion and all this function.\\\n",
    "As an anology you can imagine a signal(the inputs \"x\") that need to travel across 12 floors of a building(the transformer) at the time that the signal have reach the 12th floor is almost inexistent because in every floor it gets weaken,so to avoid this we enrich the signal in every floor to be able to reach the top of the building.\n",
    "\n",
    "\n",
    "\n",
    "This function is so extra I know,but is for me to learn it and to have a more clean \"memory map\" of what is happening in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe431c74-e901-4f2e-9ab2-9d45f5a18c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_connection(x,functionout_x):\n",
    "    return x + functionout_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c416805-c328-4655-8557-6778e35baaaa",
   "metadata": {},
   "source": [
    "Here we initialize our parameters for the Feed_Foward Network(FFN):\\\n",
    "To create the weights we create a tensor2D of random values(W1) which will have the amount of rows of the number of embeddings of the tensor that comes from below (128 embds)and the amount of neurons that we want (512 neurons(128 * 4)).\\\n",
    "Why \"*4\"? Because that's the sweet spot that the creators found in the \"Attention is all you need\" paper to do this expansion and compression in the FFN.\n",
    "\n",
    "For W2 we need to do the same but with the opossite values for the \"columns & rows\" of the tensor2D of the hidden layer of our FFN,comparison:\\\n",
    "W1 = (128,512)\\\n",
    "W2 = (512,128)\\\n",
    "Then we just use again another activation formula,and in this case I'll use He init for the hidden layer weigths because it is better for doing ReLU.\\\n",
    "And Xavier for the output layer weigths because after we do the residual connection using the output of the FFN function we going to do a Softmax and for Softmax is better(actually just standard idk if is better at this point) Xavier for it.\n",
    "\n",
    "Then for the Biases as we know we need a Bias per Neuron so that what we do,we create a tensor1D of (1,512) for the B1.\\\n",
    "And a Tensor1D of (1,128) for the B2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8b69f2-3948-4bcd-9a9f-d51d39ac98b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.randn(n_embd,n_embd * 4) * np.sqrt(2 / n_embd)\n",
    "W2 = np.random.randn(n_embd * 4,n_embd) * np.sqrt(1 / (n_embd * 4))\n",
    "B1 = np.zeros((1, n_embd * 4))\n",
    "B2 = np.zeros((1, n_embd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449db8a7-38af-4b77-bdda-d882c28f69fc",
   "metadata": {},
   "source": [
    "Here we have the Feed-Foward Network, in comparison we can see this part as the Neural Network of MNIST.\\\n",
    "What we have here is the Input that comes from below (Pos_encoding,MHA,LayerNorm,Residual_Connection) going to the hidden layer and \"mixxing\" with the W1 & B1,this creates Z1.\\\n",
    "Then Z1 does a ReLU and we get the activation of Z1 which is A1.\\\n",
    "Then A1 can do the same that previously which is \"mixxing\" but now with W2 & B2 resulting in Z2.\\\n",
    "Finally we return the values and if you notice it we don't do the A2 that we normally have to do, because before that we need to do the residual connection of this output of the function to the input of this function to just then get the A2, which will be Z2 enriched by the inputs and the result pass it through Softmax.\n",
    "\n",
    "For a more In-Deep Explanation of what is happening in this I encourage you to see my Neural Network from Scratch repository where I explain this better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6461045b-b558-4528-8db6-31af5e4c369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FFN(x,W1,B1,W2,B2):\n",
    "\n",
    "    Z1 = x @ W1 + B1\n",
    "    \n",
    "    A1 = Z1.ReLU()\n",
    "\n",
    "    Z2 = A1 @ W2 + B2\n",
    "\n",
    "    return Z1,A1,Z2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6774aa66-cc9c-4790-a47f-cfb5eb31f596",
   "metadata": {},
   "source": [
    "This is the Cross_Entropy_Loss function.\\\n",
    "It will receive the inputs which is the Tensor2D after the Softmax of the residual connection between the input and output of the FFN function, and the targets which is have the actual token that the model should have predict.\n",
    "\n",
    "First we take the shape of our inputs and then collapse the batch_size and the block_size doing a reshape to it,ending up with a (32,65)tensor,which is 32 predictions of the 65 classes that the model could predict.\n",
    "\n",
    "Then we need the one hot of the targets to be able to compare them with the inputs,so in order to do that we create a Tensor2D with the same size as the inputs but filled with zeros.\\\n",
    "Now using the fancy indexing technique we create a \"ghost tensor\" on top of our Tensor of zeros selecting the place where we have a number and once selected with the fancy indexing we replace that zero with a 1.0,creating a One_Hot with the places where was a number in the original targets now they have a 1.0.\\\n",
    "Finally we transform that numpy Tensor2D to a Tensor of the minitorch declaring that it doesn't need a gradient.\n",
    "\n",
    "Now we can have the camparison between the inputs tensor and the targets_onehot tensor and store them in a variable.\n",
    "\n",
    "Then we can do a Log() to that to increase the \"punish\" to the model on every error,but the log alone will make the numbers negative when we actually need them as positive numbers,the solution is just multiply them by -1.0 which will modify the symbol and conserving the number as it is.\n",
    "\n",
    "Once we got the logs we just need to do the mean of that,my minitorch doesn't have the mean function right now so I'll use a trick with the Sum() that we did before which is divide the logs by the amount of predictions giving us the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3340b3ea-b433-4bd2-8a55-e69d577f8105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(inputs,targets):\n",
    "\n",
    "    batch_size,block_size,classes = inputs.shape\n",
    "\n",
    "    \n",
    "    inputs_flat = inputs.reshape(batch_size * block_size,classes)\n",
    "\n",
    "    \n",
    "    oh_mask = np.zeros((batch_size * block_size,classes))\n",
    "\n",
    "    oh_mask[np.arange(batch_size * block_size),targets.astype(int)] = 1.0 #Fancy indexing for the One Hot\n",
    "\n",
    "    targets_oh = Tensor(oh_mask,requires_grad=False)\n",
    "\n",
    "\n",
    "    correct_probs = (inputs_flat * targets_oh).Sum()\n",
    "\n",
    "    \n",
    "    logs = correct_probs.Log() * -1.0\n",
    "\n",
    "    \n",
    "    return logs * (1.0 / (batch_size * block_size)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f89230-3039-44d1-a841-8554b8737cc5",
   "metadata": {},
   "source": [
    "Again create a function for this is actually not needed,but to learn how is the process that the model have to do I'll create this function and call it when needed.\n",
    "\n",
    "Because the derivative of the addition of x + y is 1 the gradient just get distributed to both of them.\\\n",
    "Like in the foward pass the residual connection have the same functionality which is \"amplify the signal\" in order to avoid the gradient vanish,because without the residual connection amplifying the signal if the backprop of the FFN (for example) pass the gradient to the backprop of the layer norm if some of the weights/data are too low, the backprop of the layer norm will make it lower and so on; Pushing the gradient to become 0 and avoiding the correct backpropagation making us that we can't train the model because the gradient ended up as nothing mid backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ef05f4-5275-468d-ae9e-50784cbbe04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_residual_connection(grad_input,grad_output):\n",
    "    return grad_input + grad_output    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecca4bf9-da20-4b95-b579-baba8fe74826",
   "metadata": {},
   "source": [
    "This is the backpropagation of the layer norm.\\\n",
    "We will take the gradient that comes from above and the cache that we have store earlier in the layer norm function of the foward pass.\\\n",
    "Next step is take the variables stored inside the cache variable and the shape of the gradient.\n",
    "\n",
    "if you see the mathematical operation that we did in the foward pass to add the gamma and beta parameters in the layer normalization you will see that the operation is similar to a layer in a NN (X @ W + B) but in this case was (X * gamma + beta) so,in therms of beta is pretty simple,to get its derivative you just need to do the same thing that you do to get the derivative of a bias,which is the summatory of all the error.\\\n",
    "Then the derivative of gamma is conceptually similar,the difference is that in the NN you get a dot product and here you do a multiplication element by element,so the derivative will be the summatory of the comparison of the gradient with the x_hat.\n",
    "\n",
    "Now dx is similar to get A1 in a NN the loss multiplied by the weights(in this case the parameter gamma).\\\n",
    "Next is \"dx\" which is the derivative to the input of the layer norm and honestly is just a mathematical formula that we traslate to code.If you want to know more about the formula you can check it out in this link: \"https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\" (copy and paste it in the browser without the quotes if it doesn't work).\\\n",
    "I'll not explain the formula because is pure mathematical logic,but to understand what is happening without the need of the link you can imagine that because is the layer_norm a process in the middle of the transformer it will have a previous function and in order to continue the backpropagation we need to keep passing the gradient and thanks to that formula the gradient can reach the input of the function and continue to the next backprop_function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc81cb5-9f7f-4349-8f9c-1be7dff49f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_LayerNorm(grad_output,cache):\n",
    "\n",
    "    x_hat, std, gamma = cache\n",
    "\n",
    "    batch_size, block_size, n_embd = grad_output.shape\n",
    "\n",
    "\n",
    "    dbeta = np.sum(grad_output, axis=(0,1))\n",
    "\n",
    "    dgamma = np.sum(grad_output * x_hat, axis=(0,1))\n",
    "\n",
    "\n",
    "\n",
    "    dx_hat = grad_output * gamma\n",
    "\n",
    "    dx = (1.0 / n_embd) * (1.0 / std) * (\n",
    "        n_embd * dx_hat -\n",
    "        np.sum(dx_hat, axis = -1, keepdims = True) -\n",
    "        x_hat * np.sum(dx_hat * x_hat, axis = -1, keepdims = True)\n",
    "    )\n",
    "\n",
    "\n",
    "    return dx, dbeta, dgamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4592834b-1006-4651-abf9-9f62f79870cd",
   "metadata": {},
   "source": [
    "This is the \"Final Boss of the Backpropagation\"(honestly I felt more difficult the backprop of the layernorm because of its mathematics in dOut,this is practically just a \"dance\" of dimensions).\\\n",
    "Because of my previous creation of the Autograd engine, I learned a lot of the back propagation of the different formulas,here we use  in a huge part the bakpropagation of the matmul which is:\\\n",
    "if Z = X @ Y \\\n",
    "The derivative of X is = grad_output @ Y.T \\\n",
    "And the derivative of Y is = X.T @ grad_output\\\n",
    "If you want to memorize this is pretty simple, just put your tensor of gradients in the same place of the foward of the variable that you want the derivative and multiply it by the other variable transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d20c75d-a4b4-4a73-a04e-b4004e295b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_MHA(grad_output, cache):\n",
    "\n",
    "    # Section 1\n",
    "    \n",
    "    x, Wq, Wk, Wv, Wo, Q, K, V, wei, probs, out_concat, head_size = cache\n",
    "\n",
    "    batch_size, block_size, n_embd = x.shape\n",
    "\n",
    "    heads_amount = Q.shape[1]\n",
    "\n",
    "\n",
    "    # Section 2\n",
    "\n",
    "    dWo\n",
    "    \n",
    "\n",
    "    \n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
