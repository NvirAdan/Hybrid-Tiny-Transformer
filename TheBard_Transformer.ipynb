{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b24cf43-f6bb-46d9-ab97-d6bf488748f3",
   "metadata": {},
   "source": [
    "Import our dependencies and the Minitorch from my github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "444d1968-4be8-4511-8dd8-6bc41fdeea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from engine.minitorch import Tensor,optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46196ca-0f6c-461f-9765-361da3050528",
   "metadata": {},
   "source": [
    "I will put a seed right now for the reproducibility of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4bab7bf-933e-4725-9c3f-3fcb1994cd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3329)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbf99ea-cc91-4740-b81b-59d18ad6a070",
   "metadata": {},
   "source": [
    "This are the variables that we need in order to create the data folder with the dataset inside,because I can't upload the repository with the dataset of Shakespeare pre-charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b9c463-6716-482f-aeb4-c4176f745042",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"data\"\n",
    "file_path = os.path.join(data_directory, \"input.txt\")\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d34f5b2-205b-43bb-b19a-3b7b92ea9a7e",
   "metadata": {},
   "source": [
    "This will create a data folder if it doesn't exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21fe0ac9-d73b-4ad2-9663-a7ac7d2959a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_directory):\n",
    "    \n",
    "    os.makedirs(data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2029296c-2484-4042-8eea-efeceb40f59b",
   "metadata": {},
   "source": [
    "if the inside of the data folder that we just created is empty we throw a request to the url for the dataset of Shakespeare of the github of Karpathy,raise for status to inform us what happened with the request and then open it indicating to the computer that need to transform it binary to avoid missinterpretations of characters and then finally write it down to the \"input.txt\" inside our data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bd91d87-5ae3-4451-9de0-cbebec12745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(file_path):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    response.raise_for_status()\n",
    "\n",
    "\n",
    "    with open(file_path, \"wb\") as f:\n",
    "\n",
    "        f.write(response.content)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12153e54-7ab3-4198-8aa7-5ef9a727c813",
   "metadata": {},
   "source": [
    "Here what we do is open the input.txt of Shakespeare preaviously stored in the data folder and read it,the encoding utf-8 is the standard to read special characters like emojis,commas,dots,etc.\\\n",
    "Then the text variable is just all that concatenated ,like a list of all Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f47f998c-44ef-4aa8-a33f-bc8849130ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f21a52b-7b45-4a47-aa01-c0d06a6e392f",
   "metadata": {},
   "source": [
    "First we convert the text into a set,this will give us every character inside the text.\\\n",
    "Then we transform that set into a list to be able to sorted in order that the same character never change its index.\\\n",
    "Lastly we can see the len of the vocabulary and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d058254-74cd-4627-afaf-ae7b4608fe56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "\n",
    "print(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c839db03-2a74-4bd3-96cc-8b8f0d571a17",
   "metadata": {},
   "source": [
    "To tokenize them we just create two dictionaries, String to Integer(stoi) and Integer to String(itos).\n",
    "\n",
    "One will have as key_value the character and as the value the index of that character in the list of chars.\n",
    "\n",
    "And the other one will have its key_value the index and as value the character of that index in the list of chars.\n",
    "\n",
    "This is exactly what we want because we use the list of characters and its indexes to transform them into integers and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f4f99df-69e2-49aa-9ece-bd8c4b6aefa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "\n",
    "itos = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f25d294-37ba-482c-9530-017d25e5e33a",
   "metadata": {},
   "source": [
    "Now here is the actual encoding and decoding.\\\n",
    "We gonna use lambda function because this is simple and with lambda we avoid writing a def just for this.\n",
    "\n",
    "encoder = here we pass to lamba a string(s) then we ask for its correspondent value inside the stoi dictionary.\n",
    "\n",
    "decoder = this will receive a list of numbers,search what character correspond to that integer inside of itos and concatenate them into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc49c95f-c212-48a6-b756-8d3808cd6720",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = lambda s:[stoi[c] for c in s]\n",
    "\n",
    "decoder = lambda l: ''.join([itos[i]for i in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571b3444-67a8-46e1-9a12-0a6281f9d1a1",
   "metadata": {},
   "source": [
    "Now that we got our encoder and decoder que can transform the text into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac633cf-f554-4669-9775-1709b1805d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.array(encoder(text), dtype=np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1be1b23-d448-47d1-9fdf-441edda5662e",
   "metadata": {},
   "source": [
    "Here we gonna take the 90% of the total data(len of total_data * 0.9).\\\n",
    "And then we use that number to divide the data into train_data and test_data.\n",
    "\n",
    "train_data being the first 90% of the data.\\\n",
    "test_data being the last 10% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeebbcaa-77d0-4afe-831f-43515c3e75f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(all_data))\n",
    "\n",
    "train_data = all_data[:n]\n",
    "test_data = all_data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112ebfce-b3ff-49dc-90b2-96d96d57201c",
   "metadata": {},
   "source": [
    "Here is the function to create batches because for training you can't pass all the data at once,explanation = Self-explanatory, just don't.\n",
    "\n",
    "For that reason we create this function passing as arguments the split which is a string.\\\n",
    "The batch_size which is one of the dimensions and the block_size which is the other dimension.\n",
    "\n",
    "First we declare the variable data, this variable will be equal to the train_data if when we pass the split(the string parameter) we said \"train\" otherwise the variable data will be equal to the test_data.\n",
    "\n",
    "Now we gonna create the randomness in the batches in order to train better the model,using numpy,we tell it to give us a random number between 0 and the total of the data minus the block size so it doesn't give us por example the last 2 tokens in the data and broke our block size.\\\n",
    "And then we also pass the batch_size to tell numpy that we want more than a single number.\n",
    "\n",
    "Next we gonna create our Tensors2D(block_size,batch_size).\\\n",
    "To create our inputs (x) we need to put them in a stack in order to create the mesh/matrix so using and excel sheet as analogy you can interpret the block size as the how large will we the rows and the batch_size as how large will be the columns(for example, if we pass block_size=8 and batch_size=4 the tensor that we gonna get is a 4x8 matrix,4 batches with a lenght of 8 values each).\\\n",
    "Once acquire the inputs, the targets(y) follow the same creation logic but +1.It will be the same size as the tensor \"X\".\n",
    "Example: if tensor start with something like  43,56,21 then tensor Y will be like 56,21,33 .\n",
    "\n",
    "Lastly we just return both the inputs and targets tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b010bd3-160b-46fe-8e72-2769459f3182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(split,batch_size,block_size):\n",
    "\n",
    "    data = train_data if split == \"train\" else test_data\n",
    "\n",
    "    idx = np.random.randint(0,len(data) - block_size,batch_size)\n",
    "\n",
    "    x = np.stack(data[i:i+block_size] for i in idx)\n",
    "\n",
    "    y = np.stack(data[i + 1 : i + block_size + 1] for i in idx)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60f744d-5ba2-4acc-a960-4e0386a5ba3c",
   "metadata": {},
   "source": [
    "Here we have our variables to be able to create the table of embedding.\\\n",
    "We will use this to transform the 65 posibles tokens into vectors of random values(one vector per char) to be able to do maths operations with the vectors.\n",
    "\n",
    "First we take the vocab_size.\\\n",
    "Then we assing a variable called numbers of embedding which is arbitrary,for a model like this I recommend 128 because is a tiny model,every of the 128 will give to the model context of what is about the token, for example, if the token 5 represent the letter \"a\" the table of embedding with each of the 128 will give us an adjective of it: \"vocal\",\"minuscule\".\n",
    "\n",
    "\n",
    "Then we initialize the variable table creating a tensor2D of random numbers,each row will represent 1 token(65 row because of the vocab_size) and every column is value of embedding for that token(128 embeddings per token).Creating a tensor2D of (65,128).\\\n",
    "We just multiply those values by the Xavier formula that is commonly used for Softmax/Sigmoid/Tanh because the randomness is too high for the model to start learning right away,it could probably(and luckily if doesn't explode)take a lot of epochs start learning.\n",
    "\n",
    "By the way,we created this Variables outside the get_embeddings function because we need to train this random values,if we initialize them inside the function they will be created every time we call the function and never train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eafe2d-b362-416e-bc53-cc72454aab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = vocab_size # Just to bring it closer we already know that they are 65 characters\n",
    "\n",
    "n_embd = 128\n",
    "\n",
    "\n",
    "token_table = np.random.randn(vocab_size,n_embd) * np.sqrt(2/n_embd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1fe52e-1777-4889-a32b-882657d42376",
   "metadata": {},
   "source": [
    "This short function is the embedding of our tokens,is short because Numpy does the hard lifting for us creating the new tensor and all that.\n",
    "\n",
    "When we call the function we pass our batches of tokens for example (4x8) and we also pass the table of embeddings of random values(65,128).\\\n",
    "What Numpy does is create a new Tensor, but this tensor is a Tensor3D.\\\n",
    "When we said use the inputs as index of table we practically say,from the embeddings table put in a new tensor the rows of every value in the block_size of the batch_size giving us a Tensor3D of (4,8,128).\\\n",
    "In an excel sheet example in this new tensor we got: the 128 embeddings columns(n_embd=128) for each row of the 8 rows(block_size=8) and once we got that tensor2D we need to stack 4 tensor2D like those(batch_size=4) on top of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663ad236-4a4f-48d4-bb2b-ceff92cb18d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  def get_embeddings(xb,token_table):\n",
    "#      return token_table[xb]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36d3229-81e9-465f-ace5-77494129fc04",
   "metadata": {},
   "source": [
    "Same as the token_table we iniliazite a tensor2D but instead of using the vocab_size we use the block_size(Also multiplying by the Xavier formula)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8de48a-b32a-4bcd-950d-906ec3415207",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_table = np.random.randn(block_size,n_embd) * np.sqrt(2/n_embd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aedcdfa-ae39-4069-bb3a-933a67694d44",
   "metadata": {},
   "source": [
    "I encourage you to learn the theory behind this positional encoding,is facinating see how words can follow a sinusoidal wave to know time and position of each embedding(maybe in future projects I'll implement this advance,more mathematical version on a transformer,which is the original way suggested by the paper \"Atenttion is all you need\",extrapolation, \"infinite\" positions),For now I'll stick with the GPT way(Learned positional embeddings)which is just another tensor like the \"embeddings_table\" that add time and position with random numbers that later on the model will learn.\\\n",
    "Because the model without the positional encoding will think that \"The dog bites the man\" is the same as \"The man bites the dog\".\n",
    "\n",
    "\n",
    "Here we take as parameters the stack batch(4,8) the token table and pos_table that we previusly have created.\n",
    "\n",
    "For future operations I'll take the shape of xb .\\\n",
    "Now to make things more modular just in case that in the future I want a block_size of 16 instead of one of 8 like right now,we create an numpy array with the length of our block_size(T=8).\n",
    "\n",
    "Then we have the token_table(65,128) but we need a tensor3D of (4,8,128 like the token_embeddings of the previous function).\\\n",
    "so we use the same fancy indexing trick to give us the tensor3D.(after a conclusion I find the previous functon redundant and I comment it, so it doesn't affect when running all the cells,just in case;But I will leave it right there because I see it as a sign of learn not as a mistake).\n",
    "\n",
    "Then this next part is just to \"name\" put \"labels\" to every row in the pos_table,for example the first row of pos_table will be named position:0(this a analogy of how the model will think) and the next row position:1, this will continue until position:7 because the block_size only have 8 positions.\\\n",
    "This will give to the model a perception of time/space because this value will be train to tell the posible position of the token after seeing all the Shakespeare train data.\n",
    "\n",
    "Next and finally is just the summ of the token_embd which give us \"characteristics\" of the token with the pos_embd that will give us \"time/space\" of the token,if you are wondering how two tensor with different sizes can do an addition to each other,the answer is : Because of numpy,numpy is doing the broadcasting so we don't have problems with the dimension,he es just doing again and again the addition of the tensor2D(pos_embd) the amount of batches of the tensor3D(4),so we ended up with a (4,8,128) tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ece254-f3ac-4d83-8aa8-6f919da0e34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(xb,token_table,pos_table):\n",
    "\n",
    "    B,T = xb.shape\n",
    "\n",
    "    pos_idx = np.arange(T)\n",
    "\n",
    "    token_embd = token_table[xb]\n",
    "\n",
    "    pos_embd = pos_table[pos_idx]\n",
    "\n",
    "    x = token_embd + pos_embd\n",
    "\n",
    "    return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
